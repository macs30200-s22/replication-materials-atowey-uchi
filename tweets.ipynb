{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Students:  \n",
    "Carmelo Micciche  \n",
    "Vadim Benichou  \n",
    "Jennifer Vial   \n",
    "Flora Attyasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jennifervial/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jennifervial/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jennifervial/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8085106382978723"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LIBRARIES\n",
    "import xmltodict\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#Sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "stopwords_nltk = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "lancaster=PorterStemmer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "nltk.downloader.download('wordnet')\n",
    "\n",
    "\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "class classification:\n",
    "    def __init__(self, path_data, path_dev):\n",
    "        self.path_data = path_data\n",
    "        self.path_dev = path_dev\n",
    "        self.data = None\n",
    "        self.data_dev = None\n",
    "        self.predict = None\n",
    "        self.accuracy = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.path_data, sep = '\\t', header = None)\n",
    "        self.data_dev = pd.read_csv(self.path_dev, sep = '\\t', header = None)\n",
    "                \n",
    "    def clean_txt(self, txt, stopwords_nltk):\n",
    "        txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf_8').get_text()\n",
    "        txt = txt.lower()\n",
    "        querywords = txt.split()\n",
    "        txt  = [word for word in querywords if word not in stopwords_nltk] \n",
    "        return txt\n",
    "    \n",
    "    def identity_tokenizer(self, text):\n",
    "        return text\n",
    "    \n",
    "    def stemSentence(self, sentence, snowball_stemmer):\n",
    "        token_words=word_tokenize(sentence)\n",
    "        stem_sentence=[]\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(snowball_stemmer.stem(word))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n",
    "    \n",
    "    def Lancaster(self, sentence, lancaster):\n",
    "        token_words=word_tokenize(sentence)\n",
    "        stem_sentence=[]\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(lancaster.stem(word))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n",
    "    \n",
    "    def correctSentence(self, sentence):\n",
    "        token_words=word_tokenize(sentence)    \n",
    "        stem_sentence=[]\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(str(TextBlob(word).correct()))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n",
    "\n",
    "    def countSlang(self, sentence, slang_words):\n",
    "        \"\"\" Input: a text, Output: how many slang words and a list of found slangs \"\"\"\n",
    "        slangCounter = 0\n",
    "        slangsFound = []\n",
    "        sentence=sentence.lower()\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for word in tokens:\n",
    "            if word in slang_words:\n",
    "                slangsFound.append(word)\n",
    "                slangCounter += 1\n",
    "        return slangCounter\n",
    "    \n",
    "    def countMultiExclamationMarks(self, sentence):\n",
    "        \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "        return len(re.findall(r\"(\\!)\\1+\", sentence))\n",
    "\n",
    "    def countMultiQuestionMarks(self, sentence):\n",
    "        \"\"\" Count repetitions of question marks \"\"\"\n",
    "        return len(re.findall(r\"(\\?)\\1+\", sentence))\n",
    "\n",
    "    def countMultiStopMarks(self, sentence):\n",
    "        \"\"\" Count repetitions of stop marks \"\"\"\n",
    "        return len(re.findall(r\"(\\.)\\1+\", sentence))\n",
    "\n",
    "    def countElongated(self, sentence):\n",
    "        \"\"\" Input: a text, Output: how many words are elongated \"\"\"\n",
    "        regex = re.compile(r\"(.)\\1{2}\")\n",
    "        return len([word for word in sentence.split() if regex.search(word)])\n",
    "\n",
    "    def countEmoticons(self, sentence):\n",
    "        return len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', sentence))\n",
    "    \n",
    "    def addNotTag(self, sentence):\n",
    "        l=['no', 'never', 'not']\n",
    "        #sentence=word_tokenize(sentence) \n",
    "        sent = []\n",
    "        s=0\n",
    "        for word in sentence:\n",
    "            if word in l:\n",
    "                s=s+1\n",
    "                sent.append(word)\n",
    "                sent.append(\" \")\n",
    "            else:\n",
    "                sent.append(word)\n",
    "                sent.append(\" \")\n",
    "        sent = sent[:-1]\n",
    "        return  s\n",
    "    \n",
    "    def addCapTag(self, sentence):\n",
    "        \"\"\" Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_ \"\"\"\n",
    "        sentence=word_tokenize(sentence) \n",
    "        sent = []\n",
    "        s=0\n",
    "        for word in sentence:\n",
    "            if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
    "                s=s+1\n",
    "                sent.append(word)\n",
    "                sent.append(\" \")\n",
    "            else:\n",
    "                sent.append(word)\n",
    "                sent.append(\" \")\n",
    "        sent = sent[:-1]\n",
    "        return  s\n",
    "    \n",
    "    def train(self, stopwords_nltk, snowball_stemmer, lancaster, slang_words, sid):\n",
    "        self.load_data()\n",
    "        \n",
    "        data = self.data\n",
    "        data_dev = self.data_dev\n",
    "        \n",
    "        vectorizer = HashingVectorizer(tokenizer=self.identity_tokenizer, lowercase = False, n_features=5000)\n",
    "        data_reg = data.copy()\n",
    "        data_test_reg = data_dev.copy()\n",
    "                                      \n",
    "        #Rename columns \n",
    "        data_reg.columns = ['positivity', 'type', 'word', 'emplacement', 'sentence']\n",
    "        data_test_reg.columns = ['positivity', 'type', 'word', 'emplacement', 'sentence'] \n",
    "\n",
    "        #Drop word and emplacement\n",
    "        data_reg = data_reg.drop(['word', 'emplacement'], axis = 1)\n",
    "        data_test_reg = data_test_reg.drop(['word', 'emplacement'], axis = 1)\n",
    "\n",
    "        #Categories to features\n",
    "        data_reg = pd.concat([data_reg.drop(['type'], axis = 1), pd.get_dummies(data_reg['type'])], axis = 1)\n",
    "        data_test_reg = pd.concat([data_test_reg.drop(['type'], axis = 1), pd.get_dummies(data_test_reg['type'])], axis = 1)\n",
    "\n",
    "        #Emoticons\n",
    "        data_reg['Emoticons'] = data_reg['sentence'].apply(self.countEmoticons)\n",
    "        data_test_reg['Emoticons'] = data_test_reg['sentence'].apply(self.countEmoticons)\n",
    "\n",
    "        #Exclamation Marks\n",
    "        #data_reg['ExclamationMarks'] = data_reg['sentence'].apply(self.countMultiExclamationMarks)\n",
    "        #data_test_reg['ExclamationMarks'] = data_test_reg['sentence'].apply(self.countMultiExclamationMarks)\n",
    "\n",
    "        #QuestionMarks\n",
    "        #data_reg['QuestionMarks'] = data_reg['sentence'].apply(self.countMultiQuestionMarks)\n",
    "        #data_test_reg['QuestionMarks'] = data_test_reg['sentence'].apply(self.countMultiQuestionMarks)\n",
    "\n",
    "        #StopMarks\n",
    "        data_reg['StopMarks'] = data_reg['sentence'].apply(self.countMultiStopMarks)\n",
    "        data_test_reg['StopMarks'] = data_test_reg['sentence'].apply(self.countMultiStopMarks)\n",
    "\n",
    "        #Slang\n",
    "        data_reg['Slangs'] = data_reg['sentence'].apply(lambda x : self.countSlang(x, slang_words))\n",
    "        data_test_reg['Slangs'] = data_test_reg['sentence'].apply(lambda x : self.countSlang(x, slang_words))\n",
    "\n",
    "        #Correct\n",
    "        data_reg['sentence'] = data_reg['sentence'].apply(self.correctSentence)\n",
    "        data_test_reg['sentence'] = data_test_reg['sentence'].apply(self.correctSentence) #moins bon score a tester apres le clean\n",
    "\n",
    "        #Compound polarity\n",
    "        data_reg['compound'] = data_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['compound'])\n",
    "        data_test_reg['compound'] = data_test_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['compound'])\n",
    "        data_reg['polarity_blob'] = data_reg['sentence'].apply(lambda x: TextBlob(x).polarity)\n",
    "        data_test_reg['polarity_blob'] = data_test_reg['sentence'].apply(lambda x: TextBlob(x).polarity)\n",
    "\n",
    "        #posivitypolarity\n",
    "        #data_reg['posivitypolarity'] = data_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['pos'])\n",
    "        #data_test_reg['posivitypolarity'] = data_test_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['pos'])\n",
    "\n",
    "        #negativepolarity\n",
    "        #data_reg['negativepolarity'] = data_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['neg'])\n",
    "        #data_test_reg['negativepolarity'] = data_test_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['neg'])\n",
    "\n",
    "        #neutralpolarity\n",
    "        #data_reg['neutralpolarity'] = data_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['neu'])\n",
    "        #data_test_reg['neutralpolarity'] = data_test_reg['sentence'].apply(sid.polarity_scores).apply(lambda x : x['neu'])\n",
    "\n",
    "        #Stemming\n",
    "        data_reg['sentence'] = data_reg['sentence'].apply(lambda x : self.stemSentence(x, snowball_stemmer))\n",
    "        data_test_reg['sentence'] = data_test_reg['sentence'].apply(lambda x : self.stemSentence(x, snowball_stemmer)) #moins bon score a tester apres le clean\n",
    "\n",
    "        #data_reg['sentence'] = data_reg['sentence'].apply(lambda x : self.Lancaster(x, lancaster))\n",
    "        #data_test_reg['sentence'] = data_test_reg['sentence'].apply(lambda x : self.Lancaster(x, lancaster))\n",
    "\n",
    "        #CountElongated\n",
    "        data_reg['countElongated']=data_reg['sentence'].apply(self.countElongated)\n",
    "        data_test_reg['countElongated']=data_test_reg['sentence'].apply(self.countElongated)\n",
    "\n",
    "        #Majuscule word\n",
    "\n",
    "        data_reg['majusculenumber']=data_reg['sentence'].apply(self.addCapTag)\n",
    "        data_test_reg['majusculenumber']=data_test_reg['sentence'].apply(self.addCapTag)\n",
    "\n",
    "        #Cleaning sentences\n",
    "        data_reg['sentence'] = data_reg['sentence'].apply(lambda x: self.clean_txt(x, stopwords_nltk))\n",
    "        data_test_reg['sentence'] = data_test_reg['sentence'].apply(lambda x: self.clean_txt(x, stopwords_nltk))\n",
    "\n",
    "        #Count negation\n",
    "        data_reg['negations'] = data_reg['sentence'].apply(self.addNotTag)\n",
    "        data_test_reg['negations'] = data_test_reg['sentence'].apply(self.addNotTag)\n",
    "\n",
    "\n",
    "        #CountElongated\n",
    "        #data_reg['sentence']=data_reg['sentence'].apply(self.replaceElongated)\n",
    "        #data_test_reg['sentence']=data_test_reg['sentence'].apply(self.replaceElongated)\n",
    "\n",
    "        data_train = data_reg\n",
    "        data_test = data_test_reg\n",
    "\n",
    "        #Hashing : Tokenization - Train \n",
    "        response = vectorizer.fit_transform(data_train['sentence'].values)\n",
    "        data_transform_train = pd.concat([data_train.drop(['positivity', 'sentence'], axis = 1),pd.DataFrame(response.todense())], axis = 1)\n",
    "\n",
    "        #X And Y - Train\n",
    "        X_train = data_transform_train.copy()\n",
    "        Y_train = data_train[['positivity']].copy()\n",
    "        Y_train.loc[Y_train['positivity']=='positive', 'positivity'] = 1\n",
    "        Y_train.loc[Y_train['positivity']=='neutral', 'positivity'] = 0\n",
    "        Y_train.loc[Y_train['positivity']=='negative', 'positivity'] = 2\n",
    "        Y_train = Y_train.astype('int')\n",
    "\n",
    "        #Hashing : Tokenization - Test\n",
    "        response = vectorizer.transform(data_test['sentence'].values)\n",
    "        data_transform_test = pd.concat([data_test.drop(['positivity', 'sentence'], axis = 1),pd.DataFrame(response.todense())], axis = 1)\n",
    "\n",
    "        #X and Y Test\n",
    "        X_test = data_transform_test.copy()\n",
    "        Y_test = data_test[['positivity']].copy()\n",
    "        Y_test.loc[Y_test['positivity']=='positive', 'positivity'] = 1\n",
    "        Y_test.loc[Y_test['positivity']=='neutral', 'positivity'] = 0\n",
    "        Y_test.loc[Y_test['positivity']=='negative', 'positivity'] = 2\n",
    "        Y_test = Y_test.astype('int')\n",
    "\n",
    "        #Logistic Regression\n",
    "        grid={\"C\":np.logspace(-3,5,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "        logreg=LogisticRegression()\n",
    "        logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "        logreg_cv.fit(X_train.values,Y_train.values.reshape(-1))\n",
    "        res = logreg_cv.predict(X_test.values)\n",
    "        acc = accuracy_score(Y_test.values.reshape(-1), res)\n",
    "        self.predict = res \n",
    "        self.accuracy = acc\n",
    "        \n",
    "\n",
    "path_data = '/Users/jennifervial/Desktop/exercise2/data/traindata.csv'\n",
    "path_dev = '/Users/jennifervial/Desktop/exercise2/data/devdata.csv'\n",
    "model = classification(path_data, path_dev)\n",
    "model.train(stopwords_nltk, snowball_stemmer, lancaster, slang_words, sid)\n",
    "model.accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
